{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANUCnCbfwE3h",
        "outputId": "bcc3f9dc-f822-4c6f-bce4-cb9839ad3e0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 94 unique tokens.\n",
            "Shape of data tensor: (15, 20)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 20, 100)           100000    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 64)                42240     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 142305 (555.88 KB)\n",
            "Trainable params: 142305 (555.88 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6928 - accuracy: 0.4167 - val_loss: 0.6882 - val_accuracy: 0.6667\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.6879 - accuracy: 0.7500 - val_loss: 0.6844 - val_accuracy: 0.6667\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.6830 - accuracy: 0.6667 - val_loss: 0.6805 - val_accuracy: 0.6667\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.6777 - accuracy: 0.5833 - val_loss: 0.6763 - val_accuracy: 0.6667\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.6721 - accuracy: 0.5833 - val_loss: 0.6716 - val_accuracy: 0.6667\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.6657 - accuracy: 0.5833 - val_loss: 0.6664 - val_accuracy: 0.6667\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.6586 - accuracy: 0.5833 - val_loss: 0.6605 - val_accuracy: 0.6667\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.6506 - accuracy: 0.5833 - val_loss: 0.6539 - val_accuracy: 0.6667\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.6415 - accuracy: 0.5833 - val_loss: 0.6464 - val_accuracy: 0.6667\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.6313 - accuracy: 0.5833 - val_loss: 0.6380 - val_accuracy: 0.6667\n",
            "1/1 [==============================] - 1s 652ms/step\n",
            "Sentence: \"I loved the movie, it was amazing!\"\n",
            "Predicted sentiment: Positive (Probability: 0.5873)\n",
            "\n",
            "Sentence: \"The acting was dreadful, I regret watching it.\"\n",
            "Predicted sentiment: Positive (Probability: 0.5770)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Example text data\n",
        "texts = [\n",
        "    \"The movie was absolutely brilliant, loved every minute of it!\",\n",
        "    \"The plot was engaging and the characters were well-developed.\",\n",
        "    \"I couldn't stand the terrible dialogue and wooden acting.\",\n",
        "    \"The cinematography was stunning, but the story fell flat for me.\",\n",
        "    \"Overall, I found the film to be disappointing and uninspired.\",\n",
        "    \"I was pleasantly surprised by how much I enjoyed the movie.\",\n",
        "    \"The acting was superb, especially from the lead actor.\",\n",
        "    \"The special effects were impressive, but the pacing was off.\",\n",
        "    \"This movie is a must-watch for any film enthusiast.\",\n",
        "    \"I found it hard to connect with any of the characters.\",\n",
        "    \"The movie kept me on the edge of my seat from start to finish.\",\n",
        "    \"I couldn't stop laughing at the hilarious jokes throughout.\",\n",
        "    \"The soundtrack added so much depth to the emotional scenes.\",\n",
        "    \"The ending was predictable and left me feeling unsatisfied.\",\n",
        "    \"I was deeply moved by the powerful performances of the cast.\"\n",
        "]\n",
        "\n",
        "labels = np.array([1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1])\n",
        "\n",
        "\n",
        "# Tokenizing text\n",
        "max_words = 1000  # Maximum number of words to tokenize\n",
        "maxlen = 20  # Maximum length of input sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "# Padding sequences to have the same length\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "print('Shape of data tensor:', data.shape)\n",
        "\n",
        "\n",
        "# Building RNN model\n",
        "embedding_dim = 100  # Dimension of word embeddings\n",
        "lstm_units = 64  # Number of units in LSTM layer\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model.add(LSTM(lstm_units))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# Training the model\n",
        "model.fit(data, labels, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Example new sentences for prediction\n",
        "new_texts = ['I loved the movie, it was amazing!',\n",
        "             'The acting was dreadful, I regret watching it.']\n",
        "\n",
        "# Tokenizing and padding new text data\n",
        "new_sequences = tokenizer.texts_to_sequences(new_texts)\n",
        "new_data = pad_sequences(new_sequences, maxlen=maxlen)\n",
        "\n",
        "# Predicting sentiment for new sentences\n",
        "predictions = model.predict(new_data)\n",
        "\n",
        "# Displaying the predictions\n",
        "for i, text in enumerate(new_texts):\n",
        "    sentiment = 'Positive' if predictions[i] > 0.5 else 'Negative'\n",
        "    print(f'Sentence: \"{text}\"')\n",
        "    print(f'Predicted sentiment: {sentiment} (Probability: {predictions[i][0]:.4f})')\n",
        "    print()\n"
      ]
    }
  ]
}